# WAVENET: A GENERATIVE MODEL FOR RAW AUDIO

https://arxiv.org/pdf/1609.03499


---

## 1. WaveNet의 핵심 아이디어

기존의 음성 합성(TTS)은 음성을 주파수 등 여러 매개변수로 변환하고, 이 매개변수를 예측한 뒤 다시 음성으로 합성하는 다단계 방식을 사용했음. 

-> 이 과정에서 음질이 손상되거나 부자연스러운 소리가 발생하는 문제가 있었음.

WaveNet은 이런 중간 과정 없이, 오디오 파형의 다음 샘플 값을 이전 샘플 값들로부터 직접 예측하는 **autoregressive** 방식을 사용함. 
"지금까지의 소리"를 바탕으로 "바로 다음 순간의 소리"를 예측하는 과정을 반복하여 전체 음성을 생성

#### **수학적 원리: 조건부 확률 모델**

WaveNet은 전체 오디오 파형 $x = \{x_1, x_2, ..., x_T\}$의 결합 확률을 아래와 같이 조건부 확률의 곱으로 모델링

$$p(x) = \prod_{t=1}^{T} p(x_t | x_1, ..., x_{t-1})$$

* 이 식은 $t-1$ 시점까지의 오디오 샘플($x_1, ..., x_{t-1}$)이 주어졌을 때,
* $t$ 시점의 샘플 값 $x_t$가 나타날 조건부 확률을 계속 곱해 전체 파형의 확률을 계산하는 것을 의미함.

---

### 2. WaveNet의 핵심 구조

이 아이디어를 구현하기 위해 CNN을 기반으로 몇 가지 핵심적인 구조를 도입.

#### **Causal Convolutions (인과적 합성곱)**

<img width="607" height="211" alt="image" src="https://github.com/user-attachments/assets/cdd772ba-b2f3-4f96-9aaf-8371947cc320" />


* 모델이 $t$ 시점의 값을 예측할 때, 미래 시점인 $t+1, t+2$ 등의 데이터를 참조하지 못하도록 막은 구조임.
* 이 구조는 출력이 항상 이전 시간의 입력에만 의존하여 시간의 순서를 위배하지 않음.

#### **Dilated Causal Convolutions (확장된 인과적 합성곱)**

<img width="609" height="207" alt="image" src="https://github.com/user-attachments/assets/be9710a3-7299-43e0-a34f-d4ab396aaf2b" />


* 음성 데이터는 초당 샘플 수가 매우 많아 일반적인 합성곱으로는 긴 시간의 의존성을 학습하기 어려움. 이 문제를 해결하기 위해 도입
* 합성곱 필터를 적용할 때, 입력 값의 일부를 건너뛰며(skipping) 더 넓은 영역을 보도록 설계됨.
* 층이 깊어질수록 건너뛰는 간격(dilation)을 $1, 2, 4, 8, ..., 512$ 와 같이 2의 거듭제곱으로 늘렸다가 다시 반복하는 방식을 사용
* 이 구조 덕분에 층을 많이 쌓지 않고도 **receptive field을 기하급수적으로 늘려** 긴 시간의 패턴을 효율적으로 학습할 수 있었음.

#### **기타 주요 구성 요소**

* **Softmax 분포 및 μ-law Companding**:
    * 오디오는 보통 16비트로 표현 -> 계산량이 너무 많음.
    * 이를 해결하기 위해 **μ-law companding** 변환을 적용하여 데이터를 256개의 값으로 비선형 양자화
    * 256개로 줄어든 값을 예측하기 위해 최종 출력층으로 Softmax를 사용
* **Gated Activation Units**:
    * 활성화 함수로 ReLU 대신 Gated Activation Unit을 사용함.
    > 이는 tanh와 sigmoid 함수를 요소별 곱셈으로 결합한 형태로, 오디오 신호 모델링에서 ReLU보다 훨씬 좋은 성능을 보임
* **Residual & Skip Connections**:
  <img width="547" height="277" alt="image" src="https://github.com/user-attachments/assets/c00163e2-f2f8-419d-a84e-59e01a4a3a39" />

    * 모델 전체에 Residual 연결과 Skip 연결을 사용했음.
    * 이를 통해 모델을 훨씬 깊게 쌓을 수 있었고 수렴 속도를 높일 수 있었음.

---

### 3. 실험 및 결과

#### **텍스트 음성 변환 (TTS)**

WaveNet의 성능이 가장 두드러지게 나타난 분야로, 기존의 최고 성능 모델들과 주관적 음질 평가(MOS: Mean Opinion Score)를 통해 비교했음.

**MOS(평균 의견 점수) 결과**

<img width="710" height="203" alt="image" src="https://github.com/user-attachments/assets/4192e719-c2fa-44bd-9cd0-e3cacaedc09a" />

> WaveNet의 MOS 점수는 기존 모델들보다 높으며, 실제 사람의 음성 점수에 근접
> 
> 청취자들에게 두 가지 다른 시스템으로 생성된 음성을 들려주고 어느 쪽이 더 자연스러운지 선택하게 했을 때, WaveNet이 기존 최고 성능 모델보다 높은 선호도를 보임

#### **기타 응용 분야**

* **다중 화자 음성 생성**: 단일 WaveNet 모델이 여러 화자의 특성을 모두 학습할 수 있었음. 
* **음악 생성**: 피아노 연주 데이터셋 등을 학습하여 새롭고 사실적인 음악 조각을 생성하는 데 성공함.
* **음성 인식**: 생성 모델임에도 불구하고, 구조를 일부 수정하여 음성 인식 작업에 적용한 결과, 좋은 성능을 달성함.

---

### 4. 결론 및 의의

* **WaveNet**은 원시 오디오 파형을 직접 모델링하는 새로운 접근법을 제시했음.
* **Dilated Causal Convolution**을 통해 긴 시간의 의존성을 효율적으로 학습하는 방법을 증명함.
* TTS 분야에서 기존 시스템들을 압도하는, 사람에 가까운 자연스러운 음질을 달성하며 분야의 수준을 크게 끌어올렸음.
* 음악 생성 및 음성 인식 등 다양한 오디오 관련 작업에도 적용될 수 있는 일반적이고 유연한 프레임워크임을 보여줌.
